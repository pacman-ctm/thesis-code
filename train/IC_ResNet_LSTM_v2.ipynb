{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HChDZSc6hZe0"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GStRpFQfYQmc",
        "outputId": "4aa8af7f-492f-4581-90c8-23d775e1ec3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/IC/ResNet"
      ],
      "metadata": {
        "id": "d4ph0hgLY_VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7DFPf00Yxii"
      },
      "outputs": [],
      "source": [
        "!pip install transformers > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYAEq4UlhXVt",
        "outputId": "16234b7e-1990-4d76-e35e-055862b59998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-xJoBvzwQKgJjPzHb3fq1sFicwyIisx7\n",
            "To: /content/data_v1.zip\n",
            "100% 640M/640M [00:02<00:00, 275MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Tiny dataset\n",
        "# !gdown https://drive.google.com/uc?id=1qYPCnXXxjEcHEg3tLGt3fDkd2ialAgS4\n",
        "import os\n",
        "# Full dataset with jpeg\n",
        "!gdown https://drive.google.com/uc?id=1-xJoBvzwQKgJjPzHb3fq1sFicwyIisx7\n",
        "\n",
        "# Full dataset without jpeg\n",
        "# https://drive.google.com/file/d/1gFSdm8K9SXNPXG9tQWS4bmO_nappN2AL/view?usp=share_link\n",
        "# !gdown https://drive.google.com/uc?id=1gFSdm8K9SXNPXG9tQWS4bmO_nappN2AL\n",
        "!unzip data_v1.zip -d /content/data > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_kx__wdBtvW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "data = json.load(open(\"/content/data/train_data.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1U5QboMCJ11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd28250-a228-41df-b6dd-d2cc0e1895cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'image_id': 2,\n",
              " 'caption': 'ba chiếc thuyền đang di chuyển ở trên con sông',\n",
              " 'segment_caption': 'ba chiếc thuyền đang di_chuyển ở trên con sông'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data['annotations'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXXEvVGCEXdy"
      },
      "source": [
        "#Vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "def set_random_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "metadata": {
        "id": "7_SuiLm7foKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_random_seed(10)"
      ],
      "metadata": {
        "id": "XhLXM7sCfpM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhEj46m3EXT8"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import itertools\n",
        "from itertools import count\n",
        "\n",
        "class IMCP_Vocab():\n",
        "  def __init__(self, texts) -> None:\n",
        "    words = list(itertools.chain(*[text.split(\" \") for text in texts]))\n",
        "    counter = Counter(words)\n",
        "    self.vocab = {key: i for i, key in zip(count(start = 4), counter.keys())}\n",
        "    self.special_ids = [0, 1, 2, 3]\n",
        "    self.max_seq_len = 256\n",
        "    self.counter = counter\n",
        "    self.special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "    for id, token in zip(self.special_ids, self.special_tokens):\n",
        "      self.vocab[token] = id\n",
        "    self.vocab = {k: v for k, v in sorted(self.vocab.items(), key=lambda x:x[1])}\n",
        "    self.id2word = {v: k for k, v in self.vocab.items()}\n",
        "    \n",
        "    self.bos_token = \"<bos>\"\n",
        "    self.eos_token = \"<eos>\"\n",
        "    self.pad_token = \"<pad>\"\n",
        "    self.unk_token = \"<unk>\"\n",
        "    \n",
        "  def get_vocab(self):\n",
        "    return self.vocab\n",
        "\n",
        "  def get_vocab_dump(self):\n",
        "    vocab = dict()\n",
        "    vocab['itos'] = list(vocab.keys())\n",
        "    vocab['stoi'] = self.vocab\n",
        "    vocab['freqs'] = dict(self.counter)\n",
        "    return vocab\n",
        "  \n",
        "  def batch_decode(self, predictions_ids):\n",
        "    preds = []\n",
        "    for seq in predictions_ids:\n",
        "        preds.append(\" \".join([self.id2word[id] for id in seq if id not in [0,1,2,3,4,5]]))\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuOXhFHpicmZ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QgcYI1_oh3x7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "class IMCP_Dataset(Dataset):\n",
        "  def __init__(self, image_path = \"/content/data/train-images\", summary_path = \"/content/data/train_data.json\", train = True):\n",
        "    super().__init__()\n",
        "    self.data = json.load(open(summary_path, \"r\"))\n",
        "    self.image_path = image_path\n",
        "    self.vocab = IMCP_Vocab(texts = [ann['segment_caption'] for ann in data['annotations']])\n",
        "    self.imgid2imgname = {entry['id']: entry['filename'] for entry in data['images']}\n",
        "    self.train = train\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data['annotations'])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    if self.train:\n",
        "      annotation = self.data['annotations'][index]\n",
        "      image_id = annotation['image_id']\n",
        "      # images = self.data['images'][index]\n",
        "      # image_id = images['id']\n",
        "      image_name = self.imgid2imgname[image_id]\n",
        "      image = Image.open(os.path.join(self.image_path, image_name)).convert('RGB')\n",
        "      caption = annotation['segment_caption']\n",
        "      return image, caption\n",
        "    else:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kColNhWzo1vb"
      },
      "outputs": [],
      "source": [
        "# class IMCP_Test_Dataset(Dataset):\n",
        "#   def __init__(self, image_path = \"/content/data/public-test-images\", summary_path = \"/content/data/test_data.json\", train = True):\n",
        "#     super().__init__()\n",
        "#     self.data = json.load(open(summary_path, \"r\"))\n",
        "#     self.image_path = image_path\n",
        "#     self.vocab = IMCP_Vocab(texts = [ann['segment_caption'] for ann in data['annotations']])\n",
        "#     self.imgid2imgname = {entry['id']: entry['filename'] for entry in data['images']}\n",
        "#     self.train = train\n",
        "\n",
        "#   def __len__(self):\n",
        "#     return len(self.data['annotations'])\n",
        "\n",
        "#   def __getitem__(self, index):\n",
        "#     if self.train:\n",
        "#       annotation = self.data['annotations'][index]\n",
        "#       image_id = annotation['image_id']\n",
        "#       # images = self.data['images'][index]\n",
        "#       # image_id = images['id']\n",
        "#       image_name = self.imgid2imgname[image_id]\n",
        "#       image = Image.open(os.path.join(self.image_path, image_name)).convert('RGB')\n",
        "#       caption = annotation['segment_caption']\n",
        "#       return image, caption\n",
        "#     else:\n",
        "#       pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zZ0iUzSHGzf0"
      },
      "outputs": [],
      "source": [
        "train_dataset = IMCP_Dataset()\n",
        "vocab = train_dataset.vocab\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])\n",
        "# test_dataset = IMCP_Test_Dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save vocab to file\n",
        "with open(\"/content/drive/MyDrive/IC/ResNet/vocab.json\", 'w+') as file:\n",
        "  json.dump(vocab.get_vocab_dump(), file, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "zClxA1twZC0U"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FQqkd0ujbL3x"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM6PRtmcJXqj"
      },
      "source": [
        "# Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "S0kofjwdJY1H"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class IMCP_Collator:\n",
        "  def __init__(self, vocab, train = True):\n",
        "    self.vocab = vocab\n",
        "    self.bos_id = self.vocab.get_vocab()['<bos>']\n",
        "    self.eos_id = self.vocab.get_vocab()['<eos>']\n",
        "    self.pad_id = self.vocab.get_vocab()['<pad>']\n",
        "\n",
        "  def tokenize_texts(self, captions):\n",
        "    raw_captions = [caption.split(\" \") for caption in captions]\n",
        "    truncated_captions = [s[:self.vocab.max_seq_len] for s in raw_captions]\n",
        "    max_len = max([len(c) for c in truncated_captions])\n",
        "\n",
        "    padded_captions = []\n",
        "    for c in truncated_captions:\n",
        "        c = [self.vocab.get_vocab()[word] for word in c]\n",
        "        seq = [self.bos_id] + c + [self.eos_id] + [self.pad_id] * (max_len - len(c))\n",
        "        padded_captions.append(seq)\n",
        "\n",
        "    padded_captions = [torch.Tensor(seq).long() for seq in padded_captions]\n",
        "    padded_captions = pad_sequence(padded_captions, batch_first=True)\n",
        "    return padded_captions\n",
        "  \n",
        "  def resize_and_stack(self, images):\n",
        "    new_size = (224, 224)\n",
        "    image_tensors = []\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    \n",
        "    for image in images:\n",
        "      img_tensor = transform(image)\n",
        "      image.close()\n",
        "      image_tensors.append(img_tensor)\n",
        "      \n",
        "    stacked = torch.stack(image_tensors)\n",
        "    return stacked\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    images = [example[0] for example in batch]\n",
        "    captions = [example[1] for example in batch]\n",
        "    return {\n",
        "        'images': self.resize_and_stack(images),\n",
        "        'captions': self.tokenize_texts(captions)\n",
        "    }\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cZrPU1ztRh8K"
      },
      "outputs": [],
      "source": [
        "collator = IMCP_Collator(vocab, train = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPJbo7ZCSGcj"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "beelDREDSD3x"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size = 16, collate_fn = collator, drop_last = True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size = 16, collate_fn = collator, drop_last = True, shuffle = False)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size = 16, collate_fn = collatorTest, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ikw0FuVuSU6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b163e0-540c-406f-89a1-ceb4b31de94f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "torch.Size([16, 19])\n"
          ]
        }
      ],
      "source": [
        "for x in train_dataloader:\n",
        "  print(x['images'].shape)\n",
        "  print(x['captions'].shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7RmhCUlOina"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RZSQUdsXXhVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eea4f0e-3a0f-45e8-c834-139275eac058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 247MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Epoch 0\n",
            "Epoch [1/10], Step [1/1060], Loss: 7.7540\n",
            "Epoch [1/10], Step [101/1060], Loss: 3.9190\n",
            "Epoch [1/10], Step [201/1060], Loss: 3.1035\n",
            "Epoch [1/10], Step [301/1060], Loss: 2.7878\n",
            "Epoch [1/10], Step [401/1060], Loss: 2.2108\n",
            "Epoch [1/10], Step [501/1060], Loss: 2.7448\n",
            "Epoch [1/10], Step [601/1060], Loss: 2.0379\n",
            "Epoch [1/10], Step [701/1060], Loss: 1.8433\n",
            "Epoch [1/10], Step [801/1060], Loss: 1.4940\n",
            "Epoch [1/10], Step [901/1060], Loss: 1.7577\n",
            "Epoch [1/10], Step [1001/1060], Loss: 1.4926\n",
            "Epoch [1/10], Valid Loss: 1.7853\n",
            "Start Epoch 1\n",
            "Epoch [2/10], Step [1/1060], Loss: 1.6716\n",
            "Epoch [2/10], Step [101/1060], Loss: 1.6506\n",
            "Epoch [2/10], Step [201/1060], Loss: 1.6229\n",
            "Epoch [2/10], Step [301/1060], Loss: 1.5936\n",
            "Epoch [2/10], Step [401/1060], Loss: 1.5196\n",
            "Epoch [2/10], Step [501/1060], Loss: 2.0169\n",
            "Epoch [2/10], Step [601/1060], Loss: 1.6426\n",
            "Epoch [2/10], Step [701/1060], Loss: 1.4760\n",
            "Epoch [2/10], Step [801/1060], Loss: 1.3026\n",
            "Epoch [2/10], Step [901/1060], Loss: 1.5661\n",
            "Epoch [2/10], Step [1001/1060], Loss: 1.2588\n",
            "Epoch [2/10], Valid Loss: 1.5838\n",
            "Start Epoch 2\n",
            "Epoch [3/10], Step [1/1060], Loss: 1.4873\n",
            "Epoch [3/10], Step [101/1060], Loss: 1.4494\n",
            "Epoch [3/10], Step [201/1060], Loss: 1.4038\n",
            "Epoch [3/10], Step [301/1060], Loss: 1.4515\n",
            "Epoch [3/10], Step [401/1060], Loss: 1.3845\n",
            "Epoch [3/10], Step [501/1060], Loss: 1.7607\n",
            "Epoch [3/10], Step [601/1060], Loss: 1.5227\n",
            "Epoch [3/10], Step [701/1060], Loss: 1.3273\n",
            "Epoch [3/10], Step [801/1060], Loss: 1.1902\n",
            "Epoch [3/10], Step [901/1060], Loss: 1.4727\n",
            "Epoch [3/10], Step [1001/1060], Loss: 1.1129\n",
            "Epoch [3/10], Valid Loss: 1.5096\n",
            "Start Epoch 3\n",
            "Epoch [4/10], Step [1/1060], Loss: 1.3288\n",
            "Epoch [4/10], Step [101/1060], Loss: 1.3253\n",
            "Epoch [4/10], Step [201/1060], Loss: 1.3054\n",
            "Epoch [4/10], Step [301/1060], Loss: 1.3493\n",
            "Epoch [4/10], Step [401/1060], Loss: 1.2594\n",
            "Epoch [4/10], Step [501/1060], Loss: 1.6413\n",
            "Epoch [4/10], Step [601/1060], Loss: 1.4595\n",
            "Epoch [4/10], Step [701/1060], Loss: 1.2191\n",
            "Epoch [4/10], Step [801/1060], Loss: 1.0896\n",
            "Epoch [4/10], Step [901/1060], Loss: 1.3685\n",
            "Epoch [4/10], Step [1001/1060], Loss: 1.0414\n",
            "Epoch [4/10], Valid Loss: 1.4598\n",
            "Start Epoch 4\n",
            "Epoch [5/10], Step [1/1060], Loss: 1.2287\n",
            "Epoch [5/10], Step [101/1060], Loss: 1.2495\n",
            "Epoch [5/10], Step [201/1060], Loss: 1.1646\n",
            "Epoch [5/10], Step [301/1060], Loss: 1.2598\n",
            "Epoch [5/10], Step [401/1060], Loss: 1.1774\n",
            "Epoch [5/10], Step [501/1060], Loss: 1.5262\n",
            "Epoch [5/10], Step [601/1060], Loss: 1.3243\n",
            "Epoch [5/10], Step [701/1060], Loss: 1.0813\n",
            "Epoch [5/10], Step [801/1060], Loss: 1.0270\n",
            "Epoch [5/10], Step [901/1060], Loss: 1.2724\n",
            "Epoch [5/10], Step [1001/1060], Loss: 0.9641\n",
            "Epoch [5/10], Valid Loss: 1.4172\n",
            "Start Epoch 5\n",
            "Epoch [6/10], Step [1/1060], Loss: 1.1179\n",
            "Epoch [6/10], Step [101/1060], Loss: 1.1524\n",
            "Epoch [6/10], Step [201/1060], Loss: 1.0921\n",
            "Epoch [6/10], Step [301/1060], Loss: 1.1644\n",
            "Epoch [6/10], Step [401/1060], Loss: 1.1064\n",
            "Epoch [6/10], Step [501/1060], Loss: 1.4272\n",
            "Epoch [6/10], Step [601/1060], Loss: 1.2529\n",
            "Epoch [6/10], Step [701/1060], Loss: 1.0022\n",
            "Epoch [6/10], Step [801/1060], Loss: 0.9450\n",
            "Epoch [6/10], Step [901/1060], Loss: 1.1644\n",
            "Epoch [6/10], Step [1001/1060], Loss: 0.8836\n",
            "Epoch [6/10], Valid Loss: 1.3795\n",
            "Start Epoch 6\n",
            "Epoch [7/10], Step [1/1060], Loss: 1.0283\n",
            "Epoch [7/10], Step [101/1060], Loss: 1.0407\n",
            "Epoch [7/10], Step [201/1060], Loss: 1.0275\n",
            "Epoch [7/10], Step [301/1060], Loss: 1.1375\n",
            "Epoch [7/10], Step [401/1060], Loss: 1.0466\n",
            "Epoch [7/10], Step [501/1060], Loss: 1.3100\n",
            "Epoch [7/10], Step [601/1060], Loss: 1.2178\n",
            "Epoch [7/10], Step [701/1060], Loss: 0.9486\n",
            "Epoch [7/10], Step [801/1060], Loss: 0.8774\n",
            "Epoch [7/10], Step [901/1060], Loss: 1.1265\n",
            "Epoch [7/10], Step [1001/1060], Loss: 0.8246\n",
            "Epoch [7/10], Valid Loss: 1.3547\n",
            "Start Epoch 7\n",
            "Epoch [8/10], Step [1/1060], Loss: 0.9547\n",
            "Epoch [8/10], Step [101/1060], Loss: 0.9849\n",
            "Epoch [8/10], Step [201/1060], Loss: 0.9617\n",
            "Epoch [8/10], Step [301/1060], Loss: 1.0684\n",
            "Epoch [8/10], Step [401/1060], Loss: 1.0154\n",
            "Epoch [8/10], Step [501/1060], Loss: 1.2410\n",
            "Epoch [8/10], Step [601/1060], Loss: 1.1653\n",
            "Epoch [8/10], Step [701/1060], Loss: 0.9242\n",
            "Epoch [8/10], Step [801/1060], Loss: 0.8391\n",
            "Epoch [8/10], Step [901/1060], Loss: 1.0579\n",
            "Epoch [8/10], Step [1001/1060], Loss: 0.7883\n",
            "Epoch [8/10], Valid Loss: 1.3314\n",
            "Start Epoch 8\n",
            "Epoch [9/10], Step [1/1060], Loss: 0.9030\n",
            "Epoch [9/10], Step [101/1060], Loss: 0.9724\n",
            "Epoch [9/10], Step [201/1060], Loss: 0.9145\n",
            "Epoch [9/10], Step [301/1060], Loss: 0.9997\n",
            "Epoch [9/10], Step [401/1060], Loss: 0.9768\n",
            "Epoch [9/10], Step [501/1060], Loss: 1.1984\n",
            "Epoch [9/10], Step [601/1060], Loss: 1.1204\n",
            "Epoch [9/10], Step [701/1060], Loss: 0.8943\n",
            "Epoch [9/10], Step [801/1060], Loss: 0.8286\n",
            "Epoch [9/10], Step [901/1060], Loss: 1.0204\n",
            "Epoch [9/10], Step [1001/1060], Loss: 0.7787\n",
            "Epoch [9/10], Valid Loss: 1.3155\n",
            "Start Epoch 9\n",
            "Epoch [10/10], Step [1/1060], Loss: 0.8830\n",
            "Epoch [10/10], Step [101/1060], Loss: 0.9471\n",
            "Epoch [10/10], Step [201/1060], Loss: 0.9017\n",
            "Epoch [10/10], Step [301/1060], Loss: 0.9790\n",
            "Epoch [10/10], Step [401/1060], Loss: 0.9450\n",
            "Epoch [10/10], Step [501/1060], Loss: 1.1691\n",
            "Epoch [10/10], Step [601/1060], Loss: 1.1191\n",
            "Epoch [10/10], Step [701/1060], Loss: 0.8824\n",
            "Epoch [10/10], Step [801/1060], Loss: 0.8236\n",
            "Epoch [10/10], Step [901/1060], Loss: 1.0087\n",
            "Epoch [10/10], Step [1001/1060], Loss: 0.7758\n",
            "Epoch [10/10], Valid Loss: 1.3102\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from transformers import get_scheduler, get_cosine_schedule_with_warmup\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load pre-trained RestNet101 model\n",
        "encoder = models.resnet101(pretrained=True).to(device)\n",
        "\n",
        "# Remove the last layer of the model\n",
        "modules = list(encoder.children())[:-1]\n",
        "encoder = nn.Sequential(*modules)\n",
        "\n",
        "# Freeze the parameters of the model\n",
        "for param in encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define LSTM decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, feature_size, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + feature_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, features, captions):\n",
        "        captions = captions[:, :-1]\n",
        "        embeddings = self.embed(captions)\n",
        "        features = features.squeeze().unsqueeze(1).repeat(1, embeddings.size(1), 1)\n",
        "        embeddings = torch.cat((features, embeddings), 2)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "# Define hyperparameters\n",
        "num_epochs = 10\n",
        "embed_size = 256\n",
        "feature_size = 2048\n",
        "hidden_size = 512\n",
        "vocab_size = len(collator.vocab.get_vocab()) + 5\n",
        "num_layers = 1\n",
        "total_step = num_epochs * len(train_dataloader)\n",
        "\n",
        "\n",
        "# Initialize decoder\n",
        "decoder = Decoder(feature_size, embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# params = list(decoder.parameters())\n",
        "# optimizer = torch.optim.Adam(params, lr=0.001)\n",
        "\n",
        "# Creating optimizer and lr schedulers\n",
        "param_optimizer = list(decoder.parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "# ]\n",
        "optimizer = torch.optim.AdamW(param_optimizer, lr=0.001, weight_decay=0.001)\n",
        "# optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=0.001, eps=1e-6)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = 0.1 * total_step, num_training_steps = total_step)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Start Epoch {epoch}\")\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        # Move data to GPU\n",
        "        images = data['images'].to(device)\n",
        "        captions = data['captions'].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions)\n",
        "        loss = criterion(outputs.permute(0, 2, 1), captions[:, 1:])\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # Print loss\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_dataloader), loss.item()))\n",
        "            \n",
        "    valid_loss = []\n",
        "    for i, data in enumerate(valid_dataloader):\n",
        "        # Move data to GPU\n",
        "        images = data['images'].to(device)\n",
        "        captions = data['captions'].to(device)\n",
        "        # Forward pass\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions)\n",
        "        loss = criterion(outputs.permute(0, 2, 1), captions[:, 1:])\n",
        "        valid_loss.append(loss.item())\n",
        "    \n",
        "    print('Epoch [{}/{}], Valid Loss: {:.4f}'.format(epoch+1, num_epochs, np.mean(valid_loss)))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8ohrPGcEz9VX"
      },
      "outputs": [],
      "source": [
        "torch.save(encoder.state_dict(), '/content/drive/MyDrive/IC/ResNet/encoder.pth')\n",
        "torch.save(decoder.state_dict(), '/content/drive/MyDrive/IC/ResNet/decoder.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}